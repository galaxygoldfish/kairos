Big Ideas: Data Loss, Bandpass Filtering, Data Segmentation

Several Changes:
    - diminishing DATA LOSS
        - sample dependent data acquisition
        - increasing max_chunklen in StreamInlet (larger chunks, reducing chance of buffer, slight
            increase in latency)
        - changed SHIFT_LENGTH from 0.1 to 0.25 (increasing makes it less likely to miss samples
            due to processing delays)
    - filtering eeg data to 8-30 (gamma is too hard to detect with Muse because of physical artifacts)
    - live version of data vs. filtered
        - filter the data at the end for best results
    - new csv's

data loss is something that we didn't think about before and it's important because if we don't,
we could have false frequencies in the data. this was fixed by making the record_live method
sample dependent instead of time dependent. also had to tweak other variables (max_chunklen).

started filtering the data to the brain wave range we want for "locked-in" waves. realized these
graphs look different to the ones we were dealing with before because of the bandpass range. also
realized that filtering the data live could make the data inaccurate. therefore, the live graph
is the unfiltered data which includes noise and everything, while the filtered data graph is
filtered. both felt important to keep just so we can see the data live and so we can still have
the nice data at the end (especially in the form of csv's)

there are 2 csv's: the time one with the data from each channel with its respective timestamp and
a flattened formatted csv that is ready to be used for machine learning (according to chatgpt).
the first csv is kind of unneccessary, but it might be useful later. (can definitely be deleted)
the second one, chatgpt said it would be the best for machine learning purposes. the csv is
formatted in a way where each row contains the data from all channels during a segment (in this
case, 2 seconds). this segmentation is helpful because it allows for more samples of data for
the machine learning model. instead of the ML model having one sample for a 30 second data stream,
it has 15 samples. it is also better to find subtle changes and differences that aren't possible
without segmentation.